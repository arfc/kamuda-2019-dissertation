\chapter{Conclusions and Future Work}

\section{General Conclusions}

This dissertation demonstrates a method to optimize machine learning architectures for an isotope classification task and a uranium enrichment regression task using low-resolution gamma-ray spectroscopy. In an attempt to shed light on a black box, CNN and DNN hyperparameters from random searches were analyzed with respect to testing set F1 score. In particular, it was found that $sqrt-max$ scaling was often a preferred preprocessing method for each model. Future experiments into machine learning architectures for gamma-ray spectroscopy should be guided by the hyperparameter bounds and discussion from this section. The method demonstrates that more complicated spectroscopic tasks, such as quantifying uranium enrichment, require more complicated machine learning architectures and datasets with a large range of simulated parameters.

This work also demonstrates a general method to construct a dataset for radioactive source interdiction. Experiments were created to test how each ANN's performance was effected by changing simulated and physical parameters that distort gamma-ray spectra. These experiments were performed for two source interdiction training datasets, the first composed of a narrow range and the second composed of a wider range of simulated parameters. From these experiments we observe that convolutional models show the most promise for source interdiction, often achieving the highest F1 score on simulated datasets and high posterior probabilities for correct isotopes in measured data. Overall, machine learning models show good performance on a wide range of detector calibrations when tested against both simulated and measured spectra. This performance makes machine learning models good candidates for use on handheld RIID detectors where the calibration is often unreliable.

We also observe that including shielding in the training dataset was necessary to correctly identify shielded isotopes when tested against both simulated and measured spectra. Autoencoders trained without shielding demonstrate some generalization capabilities when identifying measured shielded spectra. The pretrained ability to reconstruct spectra uniquely benefits the autoencoders when identifying distorted spectroscopic signals and should be investigated further for practical applications.

This work also demonstrates a general method to construct a dataset for uranium enrichment regression. While CNNs outperform DNNs in source interdiction tasks, dense models show better performance when quantifying uranium enrichment in measured spectra. This shows that the convolutional kernel sizes are inappropriately large for features in enriched uranium spectra or that CNNs are better suited to the pattern recognition problem of isotope identification. This also shows that dense networks are better suited for quantification tasks which require comparing counts in specific spectral regions. Fundamentally, this demonstrates that a machine learning model must be carefully chosen and tailored to a given spectroscopic task.





\section{Suggested Future Work}

A few things need to be added for this method to be applied in production RIID devices. Dataset simulation parameters should more accurately reflect real-world conditions. To achieve this, a more realistic background count-rate distribution and additional background templates could be added to the training dataset. Devices should also come with networks designed for specific background environments, such as the background expected in a certain city or in different geological areas. Model source strengths should be based on expected count rates from each source in a range of activities expected. Additional simulated NaI crystal variations would need to be added to the training set because manufacturing differences between NaI shape effects can affect peak-to-total ratios and detector intrinsic efficiency. Shielding should be added based on how much information content is lost from the source signal when increasing shielding is added.

% Generalization performance must also be more thoroughly measured and improved. To increase generalization, a wider range of scattering environments needs to be added to the simulated spectral templates. More detector parameters need to be varied and their effect on performance must be measured. 

% New network architectures and training hyperparameters should be explored. Novel machine learning archetectures such as inception networks and batch normalization.



In order to ensure accuracy appropriate for a commercial system, additional validation datasets must be investigated. Validation datasets are needed to investigate if certain isotope combinations could mask each other or otherwise change identification. Validation datasets are also necessary to test each algorithm's detection limits with respect to source strength and measurement time. These datasets can be used to tailor alarm thresholds for operational requirements. 



A simulated training dataset allows the same machine learning model creation process to be applied to different detector materials like plastic scintillator, CZT, and HPGe detectors. Different detector materials produce spectra with unique features and significantly different resolutions. The hyperparameter selection process outlined in this work can be repeated for these materials and optimum hyperparameters can be compared. This will help us understand how each machine learning model uses spectral features.

In addition to the deep learning algorithms presented in this thesis, more classical machine learning algorithms such as the support vector machine, random forests, and k-means should be applied to similar datasets an their performance analyzed. Performance could also be compared to models trained on feature extraction methods like autoencoders or principal component analysis.

Pretrained autoencoders could be further explored for a variety of applications. Autoencoders pretrained using low-resolution NaI spectra could be used to train networks for detectors with different resolution. Different autoencoder feature extraction processes can also be explored. Examples include: using a spectrum as input and outputting posterior probability that each channel contains a photo-peak and using a gain-shifted spectrum as input and outputting a gain-corrected spectrum. The encoding from different feature extraction methods could be fed into a DNN, support vector machine, random forests, or k-means algorithm and their performance compared.

% Finally, different machine learning models that combine time-series information, such as LSTMs, could be explored. An LSTM could be trained to identify a background


% created to identify images of  could be used for SNM identification using low-resolution gamma-ray detectors. 

% models that incorporate time series 


% A method to Siamese networks for SNM identification. Step 1, train a siamese network on simulated SNM with realistic missile shielding. Step 2, take a random warhead and call it 'golden'. Compare all other warheads to this, using only the spectrum as input. Boom, got a zero-knowledge verification alg.

% Additional datasets can be constructed for specific identification problems. For example, a dataset can be constructed to perform online uranium enrichment using NaI in fuel fabrication plants. Need to incorporate centrifuge wall thickness, can possibly use time-series data to identify when things change? May need to worry more about calibration drift due to temperature or electronics drift.



% A committee of models can be explored to incorporate dense and convolutional architectures. 

