\chapter{Conclusions and Future Work} \label{conclusion}

\section{General Conclusions}

This dissertation demonstrates a method to optimize machine learning architectures for an isotope classification task and a uranium enrichment regression task using low-resolution gamma-ray spectroscopy. This work demonstrates a clear contribution to the field of radioisotope identification via gamma-ray spectroscopy. In this work we introduce \verb|annsa|, an open source Python package designed to facilitate machine learning for gamma-ray spectroscopy tasks. 

Using \verb|annsa|, we analyzed convolutional and dense neural network architectures and delivered insights into how hyperparameters affect each model's performance. In an attempt to shed light on a black box, hyperparameters from random searches were analyzed with respect to the validation dataset's F1 score. Future experiments into machine learning architectures for gamma-ray spectroscopy should be guided by the hyperparameter bounds and discussion from this work.

Analysis from the hyperparameter search shows that dense ANNs trained using both the simple and complete training dataset obtain the best validation dataset F1 score with fewer dense layers. This shows that additional dense layers produce models susceptible to overtraining. Overtraining was also observed when the total number of dense nodes increased above 1 x 10$^{3}$. Models trained with the complete dataset show improved performance as total dense nodes increased to this limit. We also found that preprocessing by taking the $sqrt$ of the spectrum resulted in the best validation dataset F1 score for the simple and complete dense models. 

Convolutional ANNs required longer filter lengths and additional convolutional layers for best performance. The complete CNNs need longer filters than the simple CNNs for good performance on their respective validation datasets. Longer filters are needed to synthesize information in a larger area of the spectrum. Complete CNNs also required longer pooling sizes. Longer pooling sizes increase model invariance to the gain changes present in the complete training dataset.

This work also demonstrates a general method to construct training datasets for spectroscopic tasks available in \verb|annsa|. We demonstrate this dataset construction method by creating training datasets for radioactive source interdiction and uranium enrichment regression. 

In most tested source interdiction cases, we show that convolutional models outperform dense models by achieving higher F1 scores on simulated testing datasets or by achieving higher correct posterior probabilities on measured sources. Experiments were created to test how each ANN's performance was effected by changing simulated and physical parameters that distort gamma-ray spectra. These experiments were performed for two source interdiction training datasets, the first composed of a narrow range and the second composed of a wider range of simulated parameters. From these experiments we observe that convolutional models show the most promise for source interdiction, often achieving the highest F1 score on simulated datasets and high posterior probabilities for correct isotopes in measured data. Convolutional models perform better because they assume local structure in the spectrum while dense models make no assumption about spectral structure. Overall, machine learning models show good performance on a wide range of detector calibrations when tested against both simulated and measured spectra. This performance makes machine learning models good candidates for use on handheld RIID detectors where the calibration is often unreliable.

We also observe that including shielding in the classification training dataset is necessary to correctly identify shielded isotopes when tested against both simulated and measured spectra. Autoencoders trained without shielding demonstrate some generalization capabilities when identifying measured shielded spectra. The pretrained ability to reconstruct spectra uniquely benefits the autoencoders when identifying distorted spectroscopic signals and should be investigated further for practical applications. In measured spectra, we show that including shielding in the training dataset is important even for isotopes with simple spectral signals. % models struggle to perform well when identifying $^{133}$Ba. This isotope is particularly important due to the spectral region (below 400 keV) in which it's photopeaks exist. Medical sources, enriched uranium, and plutonium emit photopeaks in this region and thus can be incorrectly identified by identification algorithms.

The work demonstrates that the machine learning architectures found from the hyperparameter search tuned to a classification task can be extended to other spectroscopic tasks such as quantifying uranium enrichment. Using these architectures and a dataset constructed to perform uranium enrichment quantification, we demonstrated ANNs that can differentiate between low and high enriched uranium in measured gamma-ray spectra. Because of the greater threat posed by high enriched uranium versus low enriched uranium, differentiating these two classes using low-resolution detectors is incredibly important.

While CNNs outperform DNNs in source interdiction tasks, dense models show better performance when quantifying uranium enrichment in measured spectra. This shows that the convolutional kernel sizes are inappropriately large for features in enriched uranium spectra or that CNNs are better suited to the pattern recognition problem of isotope identification. This also shows that dense networks are better suited for quantification tasks which require comparing counts in specific spectral regions. Fundamentally, this demonstrates that a machine learning model must be carefully chosen and tailored to a given spectroscopic task.






\section{Suggested Future Work}

To implement ANNs in a high performance commercial detection system, a few improvements are required. Dataset simulation parameters should more accurately reflect real-world conditions. To achieve this, a more realistic background count-rate distribution and additional background templates should be added to the training dataset. Devices should also come with networks designed for specific background environments, such as the background expected in a certain city or in different geological areas. Modeled source strengths should be based on expected count rates from each source in a range of activities expected. Additional simulated NaI crystal variations would need to be added to the training set because manufacturing differences between NaI shape effects can affect peak-to-total ratios and detector intrinsic efficiency. Shielding should be added based on how much information content is lost from the source signal when increasing shielding is added.

In order to ensure accuracy appropriate for a commercial system, additional validation datasets must be investigated. Validation datasets are needed to investigate if certain isotope combinations could mask each other or otherwise change identification. Validation datasets are also necessary to test each algorithm's detection limits with respect to source strength and measurement time. These datasets can be used to tailor alarm thresholds for operational requirements. 

A simulated training dataset allows the same machine learning model optimization process to be applied for different detector materials like plastic scintillator, CZT, and HPGe detectors. Different detector materials produce spectra with unique features and significantly different resolutions. The hyperparameter selection process outlined in this work can be repeated for these materials and optimum hyperparameters can be compared. This will help us understand how each machine learning model uses spectral features.

In addition to the deep learning algorithms presented in this thesis, more classical machine learning algorithms such as the support vector machine, random forests, and k-means clustering should be applied to similar datasets an their performance analyzed. Performance could also be compared to models trained on feature extraction methods like autoencoders or principal component analysis.

Pretrained autoencoders could be further explored for a variety of applications. Autoencoders pretrained using low-resolution NaI spectra could be used to train networks for detectors with different resolution. Different autoencoder feature extraction processes can also be explored. Examples include: using a spectrum as input and outputting posterior probability that each channel contains a photo-peak and using a gain-shifted spectrum as input and outputting a gain-corrected spectrum. The encoding from different feature extraction methods could be fed into a DNN, support vector machine, random forests, or k-means algorithm and their performance compared.

% Finally, different machine learning models that combine time-series information, such as LSTMs, could be explored. An LSTM could be trained to identify a background


% created to identify images of  could be used for SNM identification using low-resolution gamma-ray detectors. 

% models that incorporate time series 


% A method to Siamese networks for SNM identification. Step 1, train a siamese network on simulated SNM with realistic missile shielding. Step 2, take a random warhead and call it 'golden'. Compare all other warheads to this, using only the spectrum as input. Boom, got a zero-knowledge verification alg.

% Additional datasets can be constructed for specific identification problems. For example, a dataset can be constructed to perform online uranium enrichment using NaI in fuel fabrication plants. Need to incorporate centrifuge wall thickness, can possibly use time-series data to identify when things change? May need to worry more about calibration drift due to temperature or electronics drift.



% A committee of models can be explored to incorporate dense and convolutional architectures. 

