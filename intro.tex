\chapter{Introduction}

\section{Introduction and Motivation}

Gamma-ray spectroscopy plays an important role in homeland security and nonproliferation technologies. By analyzing the gamma-ray spectrum of some material, naturally occurring radioactive material (NORM) can be distinguished from threat isotopes. Threat isotopes include undeclared industrial, medical, and special nuclear material (SNM) sources that can be used in a dirty bomb or nuclear explosive device. Detecting these materials is the first step to intercepting them and preventing nuclear material proliferation.

% Gamma-ray spectroscopy can also be used to measure uranium enrichment, an important measurement for nonproliferation and nuclear treaty verification. 

Common detection materials for commercial radioisotope identification devices (RIID) are the low-resolution sodium iodide (NaI(Tl)) \cite{Hofstadter1948}, medium-resolution cadmium zinc telluride (CZT), and high-resolution high purity germanium (HPGe). All of these materials have sufficient resolution to perform gamma-ray spectroscopy. While medium- and high-resolution materials offer better performance, they suffer from several drawbacks. These materials cost more and are difficult to manufacture in large volumes. Larger detector volumes have an increased absolute efficiency which reduces measurement times. In addition to these drawbacks, HPGe detectors require cryogenic cooling. This greatly increases the weight and cost of portable HPGe detectors. Despite the reduced resolution, NaI(Tl) is standard in the RIID industry due to its low cost, ability to be manufactured in large volumes, high intrinsic efficiency.

Reported commercial RIID performance is generally poor \cite{pibida2004,blackadar2003,blackadar2004}. One study found that seven commercial RIIDs had an average of less than 50\% correct identifications for SNM, industrial, and medical sources \cite{blackadar2003}. Another study found that a 2 mm stainless steel plate (a moderate amount of shielding) was enough to eliminate correct identifications in four commercially available RIIDs \cite{pibida2004}. Because of the importance of an alarm - and because of inadequate RIID performance - the U.S. Department of Energy (DOE) employs a team of on-call spectroscopists to resolve RIID alarms \cite{burr2009}. Because of their importance, RIID detection systems are in need of improvement.

% large CZT research \cite{Gostilo2004,Chen2018}

RIID improvements fall into two categories: improvements in the quality of the spectrum and improvements in the identification algorithms \cite{swoboda2004}. Spectral quality is largely determined by the energy resolution of the radiation detection material. Improvements in energy resolution often increase manufacturing cost and reduce feasible detector size. Active research into advanced detection materials has not yet produced a material economically competitive with NaI(Tl). Because of this, RIID improvements should focus on the identification algorithms using this industry standard material \cite{blackadar2003}.

This dissertation aims to investigate a novel identification algorithm - machine learning using simulated training data - as a possible avenue of improving the performance of RIIDs using the Ortec 905-3 2- x 2-in. NaI(Tl) cylindrical scintillation detector. Despite widespread use, NaI(Tl) detectors have several issues that complicate automated identification.

The first issue is the low resolution of NaI(Tl). The poor resolution makes some photopeaks unresolvable from each other, complicating identification.

% http://www2.pv.infn.it/~debari/doc/Flyckt_Marmonier.pdf pg.4-36
The second issue is calibration drift due to changes in environmental temperature and voltage drifts in the photomultiplier tube due to changes in count rate \cite{knoll,gilmore}. This drift affects the locations and shapes of features in a gamma-ray spectrum. To solve this, the detector must recalibrate to a reference source (possibly built-in) or a naturally occurring background source. These methods fall short for different reasons. Built-in sources need to be periodically replaced and add an unwanted signal to a spectrum. Recalibration sources can be included separately, requiring the user find this source and periodically recalibrate - complicating practical use. Calibrating from a background reference (typically the 1.460 MeV gamma-ray peak from $^{40}$K or the 2.614 MeV gamma-ray peak from the thorium decay series) requires a significant measurement time for a useful signal-to-noise ratio. Short measurement times common in homeland security measurements make this option infeasible.

The third issue is a non-linear energy response which manifests as a small second order term in the detector's calibration. While not specific to NaI(Tl) detectors, the fourth issue affecting identification algorithms is background radiation from naturally occurring radioactive material (NORM). NORM radiation can change both in intensity and composition based on location and weather. This background signal can obscure features in gamma-ray spectra.

Despite the drawbacks outlined above, our previous published work has demonstrated that ANNs are capable of identifying multiple isotopes in unknown backgrounds with a wide range of calibration settings \cite{kamudaThesis2017, kamuda2017, kamuda2018}. Furthermore, the work presented in this dissertation demonstrates that ANNs are capable of performing both classification and quantification tasks using low-resolution gamma-ray spectra.


% \section{Radioisotope Identification Device Applications}

% Statistical methods applied to gamma-ray spectroscopy algorithms in nuclear security missions \cite{Min2014}.

% \subsection{Homeland Security}

% Handheld RIIDs are used by boarder protection and law enforcement for porthole and area monitoring \cite{Hodge2007}. An example of area monitoring could be monitoring a high traffic area during a high-profile event. An example of porthole monitoring is monitoring cargo vehicles. RIIDs are typically employed in second screening activities, after a primary count rate alarm is triggered. In order to minimize traffic and ensure cargo containers leave in an economically reasonable time, screening activities are kept on the order of minutes. Another example of porthole monitoring is temporary checkpoints where traffic is slowed but not stopped. Often, the important task for devices employed in vehicle screening is determining if a primary alarm came from NORM, a legitimate industrial source, or SNM.


% \cite{fagan2012}

% GRADER PROGRAM https://www.dhs.gov/guidance-grader-program


% RIIDs also can be used to identify orphaned sources.
% https://www-sciencedirect-com.proxy2.library.illinois.edu/science/article/pii/S0969804302002221

% Pozzi great references https://www-sciencedirect-com.proxy2.library.illinois.edu/science/article/pii/S0168900217300074


% DATASET https://www-sciencedirect-com.proxy2.library.illinois.edu/science/article/pii/S0168900214012741

% \subsection{Nuclear Treaty Verification}

% Talk about Zero knowledge measurements 


\section{Neural Network History}


Scientists first theorized artificial neural networks in the 1940s as a model of how complex biological systems like neuron bundles learn and remember \cite{Pitts1943, Hebb1949}. These theories hypothesized that learning takes place by reinforcing neural connections corresponding to some beneficial behavior. The first implementation of an ANN came in 1958 in the form of a machine named The Mark I Perceptron \cite{Rosenblatt1958, Rosenblatt1962}. The Mark I Perceptron was a physical neural network whose weights were changed with motor-controlled potentiometers. The Mark I Perceptron implemented a two-class image recognition problem using a 400-pixel camera. In this context, a class represents an item in a set of mutually exclusive items (e.g. dog/cat or on/off). The device also included an algorithm (the perceptron learning algorithm) to learn optimum weights for a given task. Unfortunately, the single-layer perceptron failed in cases without linearly separable classes in the data because it was based on linear combinations of fixed basis functions \cite{Bishop2006, Minsky1969}. This realization and other perceived flaws led to a temporary decline in neural network research.

% While a single layer perceptron network could only solve linearly separable problems, in 1969 it was also found that a network with multiple layers could solve problems that are non-linearly separable \cite{Minsky1969}. 

% The perceptron algorithm was limited to a single layer network due to its activation function being a non-differentiable step function.

% Another single layer neural network design was ADALINE, created in 1960 \cite{Widrow1960}. While the ADALINE algorithm shares a similar architecture with the perceptron, its learning rule differs. It was soon shown that multiple ADALINE neurons could be stacked on top of each other, creating a MADALINE network \cite{Ridgway1962}. Due to its multilayer structure, MADALINE is able to learn non-linearly separable functions. A MADALINE network was used as an adaptive filter that removes echo from phone lines \cite{Widrow1988}.

Further advances were made when Rumelhart et al. formalized error backpropagation and it's application to training ANNs \cite{Rumelhart1986}. Error backpropogation was found to be a simple and powerful method to update an ANN to learn arbitrary functions. Error backpropogation combined with gradient descent allowed for efficient ANN training. Algorithms based on the backpropogation of error are now the most common method to train ANNs. % Additionally, it was shown that error backpropogation applied with a differentiable non-linear activation function was an extremely powerful method to train ANNs \cite{Murtagh1991}.

Currently, ANNs can solve many, diverse problems. ANNs have shown promise in everyday problems such as handwritten zip code recognition \cite{LeCun1989}, image recognition \cite{Krizhevsky2012}, and fingerprint identification \cite{Jeyanthia2015}; as well as more complicated problems such as lung cancer classification based on MRI images \cite{Selvakumari2016}, estimating surface soil moisture from high-resolution aerial images of cropland \cite{Hassan-Esfahani2015}, and stock market forecasting \cite{Rababaah2015}. 


\section{Gamma-Ray Spectroscopy for Isotope Identification and Quantification}

Rawool-Sullivan et al. identified a common workflow performed by a group of trained gamma-ray spectroscopists \cite{RawoolSullivan2010}. This workflow includes discriminating source photopeaks from the background signal, adjusting the calibration using background photopeaks, and checking for shielding effects in the low-energy photopeaks. Once the spectroscopist identifies photopeaks, they use prior knowledge or consult a database of isotope decay energies to identify isotopes in the spectrum. The researchers noted that spectroscopists often employ a mixture of factual knowledge and intuition developed from analyzing tens or hundreds of gamma-ray spectra in their analysis. The researchers also noted the difficulty in incorporating this subjective analysis into an automated algorithm.
% This is one of the main arguments for using ANNs in automated isotope ID

Many automated radioisotope identification methods are available, but few perform well given a low-resolution gamma-ray spectrum of a mixture of radioisotopes. Automated RIID methods used in such scenarios include library comparison algorithms, region of interest (ROI) algorithms, principle component analysis (PCA), and template matching. While these algorithms may work well in laboratory settings, they often offer unacceptable performance in more realistic conditions.

Library comparison algorithms attempt to match photopeak energies found in a gamma-ray spectrum with those found in a library of known isotope decay energies. Drifts and uncertainties in detector calibration can lead to misidentifying photopeaks, leading to incorrect isotope identifications \cite{burr2009}. To automate this method, a separate algorithm is required to extract photopeak centroids in the presence of calibration drift and an unknown background signal. While research on methods for photopeak extraction are ongoing \cite{mariscotti1967,DELOTTO1977,GARDNER2011}, they face difficulties when a large number of photopeaks overlap in a spectrum \cite{xiong2015}, such as when a low-resolution detector measures a mixture of radio-isotopes.

ROI algorithms define regions in a spectrum where they expect target radioisotope photopeaks. These algorithms then compare counts in these regions to a measured or expected background. Significant elevation in counts in a target isotopes ROIs indicates the presence of that isotope. Similar to library comparison algorithms, ROI algorithms operate poorly when photopeaks of different radioisotopes overlap \cite{burr2009}. Because of this, large isotope libraries perform poorly using this method. Similarly to the library comparison algorithm, calibration drift may shift photopeaks into neighboring ROIs, leading to incorrect identification. Despite drawbacks, an ROI method has been used to differentiate normally occurring radioactive material (NORM) from special nuclear material (SNM) using plastic scintillators \cite{Ely2006}.

PCA can also be applied to radioisotope identification. PCA aims to reduce a dataset's dimensionality into uncorrelated variables \cite{Jolliffe2002}. Using a subset of these principle components, the data may be represented in a reduced space of orthogonal bases that contains most of the information present in the original data. Isotopes in the the transformed data can then be clustered using methods like K-means, Mahalanobis distance, or k-nearest neighbors \cite{Kanungo2002, Kumari2012}. PCA has been applied to isotope identification using plastic scintillators \cite{Boardman2012} and anomaly detection using both plastic scintillators and NaI(Tl) detectors \cite{runkle2006b}. Despite PCA's progress in some isotope identification problems, application using PCA to separating isotope mixtures in gamma-ray spectra could not be found.

Template matching algorithms find an example in a database of gamma-ray spectra that most closely matches a measured spectrum \cite{burr2009}. The spectral database can contain multiple detector calibration settings, shielding materials, and source-to-detector distances. Quality of fit can be measured using a hypothesis test such as chi-squared test or correlation coefficient. While a sufficiently large spectral database can theoretically be used to identify almost any measured spectrum, the drawback of this method is the time necessary to compare a measured spectrum to the library and the computer memory necessary to store said library. Despite the drawbacks, researchers have made progress applying a multiple linear regression procedure to identify mixtures of isotopes using template matching \cite{mattingly2010}.

The algorithms described previously largely incorporate book knowledge. The present work is motivated by the notion that incorporating the intuition identified by Rawool-Sullivan et al. can improve these algorithms. By carefully creating a training set of spectra and intelligently applying deep learning, a machine learning approach to automated gamma-ray spectroscopy may be succeed in marrying book knowledge and a trained spectroscopists intuition.


\section{Automated Isotope Identification Using ANNs}

A number of published papers apply ANNs to automated isotope identification. ANNs have been applied to peak fitting \cite{Abdel-Aal2002}, isotope identification \cite{Abdel-Aal1996, Medhat2012}, and activity estimation \cite{Abdel-Aal1996, Vigneron1996}. Many of these publications rely on ROI methods \cite{Pilato1999}, feature extraction \cite{Chen2009}, high-resolution gamma-ray spectra \cite{Yoshida2002}, and small libraries of isotopes. In addition, many assume perfect knowledge of detector calibration. ANN training methods created for high-resolution gamma-ray spectra may not perform well when trained using low-resolution spectra. Because of the large discrepancy in resolution, spectral features exploited by a ANN trained on high-resolution spectra would be different than an ANN trained on low-resolution spectra. In addition, ANN training that relies on ROI methods may not perform well when ROIs overlap significantly (as previously explained). Feature extraction and ROI methods may also falter when the background radiation field is unknown or the detector calibration is unreliable.  

Instead of training an ANN using predetermined ROIs or feature extraction, the present work hypothesizes that it is better to train the ANN with an entire gamma-ray spectrum. Due to perceived training issues and computational requirements associated with using the entire spectrum \cite{Pilato1999,Yoshida2002}, previous work in the gamma-ray spectroscopy community have avoided this approach. However, we have shown that training an ANN using the full spectrum can viably identify and quantify isotopes in gamma-ray spectra \cite{kamuda2017,kamudaThesis2017,kamuda2018}. Evidence in the present work also demonstrates that this method can overcome common gamma-ray spectroscopy issues like calibration shifts and identifying isotopes in spectra without clear spectral features.

% This work suggests performing feature extraction and radioisotope identification simultaneously using an ANN. Skipping automated peak-fitting routines releases the algorithm from the burden of determining proper fitting method for a variety of common challenges. For example, changes based on a given photopeaks location in another photopeak's Compton continuum will change the baseline. The second challenge is in deconvolving multiplets. Peak multiplets occur when photopeaks of a given isotope or mixture of isotopes overlap. Instead of making deterministic rules for each of these cases, a machine learning algorithm can be taught to recognize and handle them automatically.

% One identification method involves Once photopeaks are found, another algorithm is needed to measure the locations of peak centroids and additional information (peak area, area uncertainty) to identify what isotopes are present in a spectrum. This process adds computation time and again suffers from the need to be modified to handle changes in a spectrum as described above. Also, the algorithm that performs identification based on peak information must be tailored to the peak-fitting routine. This requires unique algorithms to be created for different detector materials and sizes, as they change the shape of the spectrum. Using an ANN with an appropriate training set, these problems can be avoided.

Using machine learning for isotope identification and quantification offers many advantages over methods previously explained. The problem of determining the feature extraction technique and optimal algorithm is avoided by training an algorithm to identify the important features of a gamma-ray spectrum and simultaneously perform identification or quantification. Using a machine learning approach also has an advantage in the flexibility of its training set and learning objective. Because this method uses simulated gamma-ray spectra to train the model, it does not restrict the number of isotopes allowed in the library. This allows us to cheaply generate the ANN training set using mixtures of exotic, dangerous, or short-lived isotopes that are not easily accessible. Because the training set and learning objective of a machine learning algorithm are flexible, we can train similar deep learning models to perform diverse tasks like source interdiction or uranium enrichment measurements.

% The isotope library and spectral conditions common for source interdiction with border patrol users are very different from those needed to perform enriched uranium measurements. %Because spectra can be generated with different calibrations, this method can be insensitive to a range of calibration shifts due to temperature change or operator error. This insensitivity would allow isotope identification and quantification to be performed without prior knowledge of the detectors calibration.
% ANNs using feature extraction on 3x3 NaI detector \cite{HE2018}.

\section{Chapter Conclusion}

This work addresses the how to generate a synthetic dataset of gamma-ray spectra for relevant problems in isotope identification and quantification. This work also emphasizes how various ANN architectures perform on these datasets. % Due to their operation, we expect different architectures to perform better on different datasets. % Gamma-ray spectroscopists often use intuition when identifying isotopes in spectra. ANNs mimic this abstract analysis, synthesizing features of a gamma-ray spectrum in non-intuitive ways. Exploiting this intuition may overcome common hurdles encountered by other isotope identification algorithms.

Experiments in this work fall into two categories: advanced simulated datasets and advanced machine learning architectures. The first advanced dataset is based on the the American National Standards Institute performance criteria for hand-held instruments for the detection and identification of radionuclides, ANSI N42-34-2006 \cite{ANSI}. This dataset will incorporate the effects of shielding, calibration drift, and unknown background. The second dataset will be based on a zero knowledge measurement for automated uranium enrichment calculations. We will assess the performance of models trained on both datasets using real and simulated gamma-ray spectra. The machine learning architectures explored will include a dense neural network (DNN), dense autoencoder (DAE), convolutional neural network (CNN), and convolutional autoencoder (CAE).