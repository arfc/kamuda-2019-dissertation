\relax 
\@writefile{toc}{\contentsline {chapter}{CHAPTER\ 2\hskip 1em\relax \MakeUppercase  {Theory}}{8}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Gamma-ray Spectroscopy}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Energy Deposition Mechanisms}{8}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1.1}Photoelectric Absorption}{8}}
\citation{knoll}
\citation{knoll}
\newlabel{eq:photoelectric_effect}{{2.1}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Energy dependence for gamma-ray interactions in NaI(Tl) \cite  {knoll}.\relax }}{9}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:energy_dependence_interactions}{{2.1}{9}}
\citation{knoll}
\citation{knoll}
\citation{gilmore}
\citation{gilmore}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1.2}Compton scattering}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Diagram of a Compton scattering event \cite  {knoll}.\relax }}{10}}
\newlabel{fig:compton_scatter}{{2.2}{10}}
\newlabel{eq:compton_scatter}{{2.2}{10}}
\citation{gilmore}
\citation{gilmore}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Diagram of energy absorbed in an idealized Compton scattering event \cite  {gilmore}.\relax }}{11}}
\newlabel{fig:ideal_compton}{{2.3}{11}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1.3}Pair Production}{11}}
\citation{knoll}
\citation{knoll}
\citation{knoll}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Diagram of pair production \cite  {gilmore}.\relax }}{12}}
\newlabel{fig:pair_production}{{2.4}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Theoretical spectrum from pair production \cite  {knoll}.\relax }}{12}}
\newlabel{fig:pair_production_spectra}{{2.5}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Energy Resolution}{12}}
\newlabel{subsection_energy_resolution}{{2.1.2}{12}}
\citation{RIIDMarketSurveyReport}
\citation{RIIDMarketSurveyReport}
\citation{RIIDMarketSurveyReport}
\citation{RIIDMarketSurveyReport}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces Typical energy resolutions of common gamma-ray detector types (reproduced from \cite  {RIIDMarketSurveyReport}).\relax }}{13}}
\newlabel{table:detector_resolutions}{{2.1}{13}}
\citation{KULISEK2015}
\citation{KULISEK2015}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces $^{133}$Ba spectrum measured using common detector materials \cite  {RIIDMarketSurveyReport}.\relax }}{14}}
\newlabel{fig:Ba133_spectrum_different_detector_materials_Market_Survey_Report}{{2.6}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Background Radiation}{14}}
\citation{knoll}
\citation{USGS}
\citation{USGS}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Monte Carlo simulations of background component spectra in NaI(Tl) \cite  {KULISEK2015}.\relax }}{15}}
\newlabel{fig:background_components}{{2.7}{15}}
\citation{USGS}
\citation{USGS}
\citation{USGS}
\citation{USGS}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Map of uranium concentrations in the United States (Reproduced from \cite  {USGS}).\relax }}{16}}
\newlabel{fig:USGS_u_conc}{{2.8}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Map of thorium concentrations in the United States (Reproduced from \cite  {USGS}).\relax }}{16}}
\newlabel{fig:USGS_th_conc}{{2.9}{16}}
\citation{blackadar2003}
\citation{IANAKIEV2009432}
\citation{MOSZYNSKI2006739}
\citation{CASANOVAS2012588}
\citation{CASANOVAS2012588}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces Map of potassium concentrations in the United States (Reproduced from \cite  {USGS}).\relax }}{17}}
\newlabel{fig:USGS_k_conc}{{2.10}{17}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4}Affect of Temperature on Calibration and Resolution}{17}}
\citation{MOSZYNSKI2006739}
\citation{MOSZYNSKI2006739}
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces Temperature vs relative photopeak position for an ORTEC Model 905-3 2x2 NaI(Tl) detector. Reproduced from \cite  {CASANOVAS2012588}.\relax }}{18}}
\newlabel{fig:CASANOVAS2012588}{{2.11}{18}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.12}{\ignorespaces The energy resolution of 662 keV $\gamma $-rays measured with a 25mm Ã— 30mm NaI(Tl) crystal using 2 $\mu $s and 12 $\mu $s peaking times. Reproduced from \cite  {MOSZYNSKI2006739}.\relax }}{18}}
\newlabel{fig:temp-dependence-resolution-moszynski}{{2.12}{18}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Machine Learning}{19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}General Neural Network Architecture}{19}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.13}{\ignorespaces Example ANN with input neurons $A_n$, hidden neurons $B_j$, and output neurons $C_k$.\relax }}{19}}
\newlabel{fig:Network}{{2.13}{19}}
\citation{Rumelhart1986}
\citation{Yao1999}
\citation{Fletcher2000}
\@writefile{lof}{\contentsline {figure}{\numberline {2.14}{\ignorespaces Summary of the operation of a single neuron.\relax }}{20}}
\newlabel{fig:Node}{{2.14}{20}}
\newlabel{eq:argminW_Error}{{2.3}{20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Simple Neural Network Example}{20}}
\citation{Hornik1991}
\newlabel{eq:single_layer_eq_sum}{{2.4}{21}}
\citation{Nielsen2015}
\@writefile{lof}{\contentsline {figure}{\numberline {2.15}{\ignorespaces Example of a single-layer neural network with two inputs ($x_{1}$ and $x_{2}$), three classes ($y_{1}$, $y_{2}$, $y_{3}$), and a bias neuron set to one.\relax }}{22}}
\newlabel{fig:one_layer_net}{{2.15}{22}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.16}{\ignorespaces A dataset describing a three class function. Each class is represented by a different color.\relax }}{22}}
\newlabel{fig:training_set_one_layer}{{2.16}{22}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Neural Network Training}{22}}
\newlabel{eq:train_data_D}{{2.5}{23}}
\newlabel{eq:dMSE}{{2.6}{23}}
\newlabel{dE_{MSE}/dy_i}{{2.7}{23}}
\newlabel{dy_i/dw_i}{{2.8}{23}}
\newlabel{eq:dE_MSE/db_j}{{2.9}{23}}
\newlabel{dE_{MSE}/dw_j}{{2.10}{23}}
\newlabel{dy_i/db_j}{{2.11}{23}}
\newlabel{eq:dE_{MSE}/dw_j_final}{{2.12}{24}}
\newlabel{eq:dE_{MSE}/db_j_final}{{2.13}{24}}
\newlabel{eq:update1}{{2.14}{24}}
\newlabel{eq:update2}{{2.15}{24}}
\newlabel{eq:CrossEntropy1}{{2.20}{25}}
\newlabel{eq:CrossEntropy2}{{2.21}{25}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.17}{\ignorespaces Ideal training and testing error curves.\relax }}{26}}
\newlabel{fig:training_testing_error}{{2.17}{26}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}Convolutional Neural Networks}{26}}
\citation{Lecun1998}
\citation{Lecun1998}
\citation{Lecun1998}
\citation{Hinton2006}
\citation{Erhan2010}
\citation{CHARTE2018}
\citation{Vincent2008}
\citation{Vincent2010}
\@writefile{lof}{\contentsline {figure}{\numberline {2.18}{\ignorespaces Architecture of LeNet-5, a CNN created for digit recognition. Image reproduced from \cite  {Lecun1998}\relax }}{27}}
\newlabel{fig:cnn_mnist_lecun98}{{2.18}{27}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Autoencoders}{27}}
\newlabel{Autoencoders}{{2.3}{27}}
\citation{Jolliffe2002}
\citation{Welling2007}
\citation{wiki:AutoencoderStructure}
\citation{wiki:AutoencoderStructure}
\@writefile{lof}{\contentsline {figure}{\numberline {2.19}{\ignorespaces An example of an autoencoder \cite  {wiki:AutoencoderStructure}.\relax }}{28}}
\newlabel{fig:Autoencoder_structure}{{2.19}{28}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Hyperparameters}{28}}
\citation{Yu1997}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1.1}Learning Rate}{29}}
\newlabel{LearningRateSubsection}{{2.3.1.1}{29}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.20}{\ignorespaces Example training paths for a large learning rate and a small learning rate.\relax }}{29}}
\newlabel{fig:Learning_rate_comparison}{{2.20}{29}}
\citation{nesterov1983}
\citation{Kirkpatrick1983}
\citation{ADADELTA}
\citation{Kingma2015}
\citation{lecun98}
\citation{Maas2011}
\citation{Krizhevsky2009}
\citation{Kingma2015}
\citation{Kingma2015}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1.2}Learning Momentum}{30}}
\newlabel{eq:update_momentum}{{2.22}{30}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1.3}Training Algorithms}{30}}
\newlabel{eq:adam1}{{2.23}{30}}
\newlabel{eq:adam2}{{2.24}{30}}
\newlabel{eq:adam3}{{2.25}{31}}
\newlabel{eq:adam4}{{2.26}{31}}
\newlabel{eq:adam5}{{2.27}{31}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1.4}Cost Function}{31}}
\newlabel{eq:Binary_accuracy}{{2.28}{31}}
\newlabel{eq:MSE_error}{{2.29}{31}}
\citation{Bridle1990}
\newlabel{eq:CrossEntropy}{{2.30}{32}}
\newlabel{eq:softmax}{{2.31}{32}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1.5}Weight Regularization}{32}}
\newlabel{eq:L2_Reg}{{2.32}{32}}
\citation{Srivastava2014}
\citation{Srivastava2014}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1.6}Neuron Dropout}{33}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1.7}Data Augmentation}{33}}
\citation{Bergstra2012}
\citation{Zheng2015}
\@writefile{lof}{\contentsline {figure}{\numberline {2.21}{\ignorespaces Two examples of data augmentation using an image of a cat. The image to the left is the original. The top right image is augmented using a horizontal flip. The bottom right image is augmented using blur.\relax }}{34}}
\newlabel{fig:cat}{{2.21}{34}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Hyperparameter Optimization}{34}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.22}{\ignorespaces A comparison between a grid search and a random search for hyperparameter optimization. In this example, performance is strongly tied to one hyperparameter. The green function represents the effect of an important hyperparameter on a cost function while the yellow function represents the effect for an unimportant hyperparameter. Figure reproduced from [42].\relax }}{35}}
\newlabel{fig:Bergstra12a_hyperparameter_grid_vs_random}{{2.22}{35}}
\citation{Bergstra2012}
\citation{Bergstra2012}
\citation{tensorflow2015-whitepaper}
\citation{numpy}
\citation{scipy}
\citation{mckinney-proc-scipy-2010}
\citation{scikit-learn}
\@writefile{lof}{\contentsline {figure}{\numberline {2.23}{\ignorespaces Example random efficiency curves for a neural network \cite  {Bergstra2012}.\relax }}{36}}
\newlabel{fig:Bergstra_random_efficiency_curve_DNN}{{2.23}{36}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}ANNSA}{36}}
\citation{mitchell2014}
\citation{Goorley2016}
\citation{geant4}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Dataset Generation}{37}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.24}{\ignorespaces ANNSA dataset generation process.\relax }}{37}}
\newlabel{fig:annsa_data_generation}{{2.24}{37}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Machine Learning Models}{38}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.2.1}Dense Neural Network}{38}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.25}{\ignorespaces ANNSA's DNN structure.\relax }}{38}}
\newlabel{fig:annsa_dnn}{{2.25}{38}}
\@writefile{lot}{\contentsline {table}{\numberline {2.2}{\ignorespaces ANNSA's DNN parameters.\relax }}{39}}
\newlabel{table:annsa_dnn_params}{{2.2}{39}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.2.2}Convolutional Neural Network}{39}}
\newlabel{annsa_section_conv_neural_network}{{2.4.2.2}{39}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.26}{\ignorespaces The 1D convolutional-pooling operation included in ANNSA.\relax }}{40}}
\newlabel{fig:annsa_cnn_fine}{{2.26}{40}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.27}{\ignorespaces ANNSA's CNN structure.\relax }}{40}}
\newlabel{fig:annsa_cnn}{{2.27}{40}}
\@writefile{lot}{\contentsline {table}{\numberline {2.3}{\ignorespaces ANNSA's 1D CNN parameters.\relax }}{41}}
\newlabel{table:annsa_cnn_params}{{2.3}{41}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.2.3}Dense Autoencoder}{41}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.28}{\ignorespaces ANNSA DAE structure. This example demonstrates a background subtracting denoising autoencoder.\relax }}{42}}
\newlabel{fig:annsa_dae}{{2.28}{42}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.29}{\ignorespaces An example of a DAE's encoding being loaded into a DNN created to classify spectra.\relax }}{42}}
\newlabel{fig:annsa_daednn}{{2.29}{42}}
\@writefile{lot}{\contentsline {table}{\numberline {2.4}{\ignorespaces ANNSA's DAE parameters.\relax }}{43}}
\newlabel{table:annsa_dae_params}{{2.4}{43}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.2.4}Convolutional Autoencoder}{43}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.30}{\ignorespaces ANNSA CAE structure. This example demonstrates a background subtracting denoising autoencoder.\relax }}{44}}
\newlabel{fig:annsa_cae}{{2.30}{44}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.31}{\ignorespaces ANNSA CAE structure.\relax }}{44}}
\newlabel{fig:annsa_caednn}{{2.31}{44}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}Training}{44}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Summary}{45}}
\@setckpt{theory}{
\setcounter{page}{46}
\setcounter{equation}{32}
\setcounter{enumi}{0}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{5}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{31}
\setcounter{table}{4}
\setcounter{float@type}{16}
\setcounter{lstnumber}{4}
\setcounter{algorithm}{0}
\setcounter{caption@flags}{0}
\setcounter{ContinuedFloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{parentequation}{0}
\setcounter{lstlisting}{0}
}
