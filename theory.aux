\relax 
\citation{CASANOVAS2012588}
\citation{CASANOVAS2012588}
\citation{CASANOVAS2012588}
\citation{CASANOVAS2012588}
\@writefile{toc}{\contentsline {chapter}{CHAPTER\ 2\hskip 1em\relax \MakeUppercase  {Theory}}{8}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Gamma-ray Spectroscopy}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Background effects}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Temperature vs relative photopeak position for an ORTEC Model 905-3 2x2 NaI detector \cite  {CASANOVAS2012588}.\relax }}{8}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:uranium_concentration}{{2.1}{8}}
\citation{Lecun1998}
\citation{Krizhevsky2012}
\citation{Simonyan2014}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Affect of Temperature Change on Calibration}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Temperature vs relative photopeak position for an ORTEC Model 905-3 2x2 NaI detector \cite  {CASANOVAS2012588}.\relax }}{9}}
\newlabel{fig:CASANOVAS2012588}{{2.2}{9}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Machine Learning}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Convolution Neural Networks}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Neural Network Architecture}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Example ANN with input neurons $A_n$, hidden neurons $B_j$, and output neurons $C_k$.\relax }}{10}}
\newlabel{fig:Network}{{2.3}{10}}
\citation{Rumelhart1986}
\citation{Yao1999}
\citation{Fletcher2000}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Summary of the operation of a single neuron.\relax }}{11}}
\newlabel{fig:Node}{{2.4}{11}}
\newlabel{eq:argminW_Error}{{2.1}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Simple Neural Network Example}{11}}
\citation{Hornik1991}
\newlabel{eq:single_layer_eq_sum}{{2.2}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Example of a single-layer neural network with two inputs ($x_{1}$ and $x_{2}$), three classes ($y_{1}$, $y_{2}$, $y_{3}$), and a bias neuron set to one.\relax }}{12}}
\newlabel{fig:one_layer_net}{{2.5}{12}}
\citation{Nielsen2015}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces One possible dataset describing a three class function. Each class is represented by a different color.\relax }}{13}}
\newlabel{fig:training_set_one_layer}{{2.6}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}Neural Network Training}{13}}
\newlabel{eq:dMSE}{{2.3}{13}}
\newlabel{eq:train_data_D}{{2.4}{13}}
\newlabel{dE_{MSE}/dy_i}{{2.5}{13}}
\newlabel{dE_{MSE}/dy_i}{{2.6}{14}}
\newlabel{eq:MSE_deriv}{{2.7}{14}}
\newlabel{dE_{MSE}/dy_i}{{2.8}{14}}
\newlabel{dE_{MSE}/dy_i}{{2.9}{14}}
\newlabel{dE_{MSE}/dy_i}{{2.10}{14}}
\newlabel{eq:ladidadi}{{2.11}{14}}
\newlabel{dE_{MSE}/dw_j}{{2.12}{14}}
\newlabel{dE_{MSE}/dy_i}{{2.13}{14}}
\newlabel{eq:dE_{MSE}/dw_j_final}{{2.14}{14}}
\newlabel{eq:dE_{MSE}/db_j_final}{{2.15}{14}}
\newlabel{eq:update1}{{2.16}{14}}
\newlabel{eq:update2}{{2.17}{15}}
\newlabel{eq:CrossEntropy}{{2.18}{15}}
\newlabel{eq:CrossEntropy}{{2.19}{15}}
\newlabel{eq:CrossEntropy}{{2.20}{15}}
\newlabel{eq:CrossEntropy}{{2.21}{15}}
\newlabel{eq:CrossEntropy1}{{2.22}{15}}
\newlabel{eq:CrossEntropy2}{{2.23}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Ideal training and testing error curves.\relax }}{16}}
\newlabel{fig:training_testing_error}{{2.7}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.5}Hyperparameters}{16}}
\citation{Yu1997}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.5.1}Learning Rate}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Example training paths for a large learning rate, red, and a small learning rate, green.\relax }}{17}}
\newlabel{fig:Learning_rate_comparison}{{2.8}{17}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.5.2}Learning Momentum}{17}}
\newlabel{eq:update_momentum}{{2.24}{17}}
\citation{nesterov1983}
\citation{Kirkpatrick1983}
\citation{ADADELTA}
\citation{Kingma2015}
\citation{Kingma2015}
\citation{Kingma2015}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.5.3}Training Algorithms}{18}}
\newlabel{eq:adam1}{{2.25}{18}}
\newlabel{eq:adam2}{{2.26}{18}}
\newlabel{eq:adam3}{{2.27}{18}}
\newlabel{eq:adam4}{{2.28}{18}}
\citation{Bridle1990}
\newlabel{eq:adam5}{{2.29}{19}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.5.4}Cost Function}{19}}
\newlabel{eq:Binary_accuracy}{{2.30}{19}}
\newlabel{eq:MSE_error}{{2.31}{19}}
\newlabel{eq:CrossEntropy}{{2.32}{19}}
\citation{Srivastava2014}
\citation{Srivastava2014}
\newlabel{eq:softmax}{{2.33}{20}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.5.5}Weight Regularization}{20}}
\newlabel{eq:L2_Reg}{{2.34}{20}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.5.6}Neuron Dropout}{20}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.5.7}Data Augmentation}{21}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Two examples of data augmentation using an image of a cat. The image to the left is the original. The top right image is augmented using a horizontal flip. The bottom right image is augmented using blur.\relax }}{21}}
\newlabel{fig:cat}{{2.9}{21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.6}Hyperparameter Optimization}{21}}
\citation{Bergstra2012}
\citation{Zheng2015}
\citation{Bergstra2012}
\citation{Bergstra2012}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces A comparison between a grid search and a random search for hyperparameter optimization when performance is strongly tied to one hyperparameter. The green function represents the effect of an important hyperparameter on a cost function while the yellow function represents the effect for an unimportant hyperparameter. Figure reproduced from [42].\relax }}{23}}
\newlabel{fig:Bergstra12a_hyperparameter_grid_vs_random}{{2.10}{23}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces Example Random efficiency curves for a neural network \cite  {Bergstra2012}.\relax }}{24}}
\newlabel{fig:Bergstra_random_efficiency_curve_DNN}{{2.11}{24}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Summary}{24}}
\@setckpt{theory}{
\setcounter{page}{25}
\setcounter{equation}{34}
\setcounter{enumi}{0}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{3}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{11}
\setcounter{table}{0}
\setcounter{float@type}{8}
\setcounter{algorithm}{0}
\setcounter{subfigure}{0}
\setcounter{lofdepth}{1}
\setcounter{subtable}{0}
\setcounter{lotdepth}{1}
\setcounter{parentequation}{0}
\setcounter{caption@flags}{0}
\setcounter{ContinuedFloat}{0}
}
